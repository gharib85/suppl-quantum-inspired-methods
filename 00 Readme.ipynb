{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum-inspired algorithms for numerical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this folder you will find the code that I used when writing the work [Quantum-inspired algorithms for multivariate analysis: from interpolation to partial differential equations](https://arxiv.org/abs/1909.06619). Part of this code is self-contained, implementing simple algorithms, such as the computation of entanglement measures or estimating interpolation errors; other parts depend on the [SeeMPS library](https://github.com/juanjosegarciaripoll/seemps). This notebook takes care of preparing the folder to host that and other libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of files is as follows\n",
    "\n",
    "- [00 Readme.ipynb](00%20Readme.ipynb). This file you are reading.\n",
    "- [01 Exact samplings.ipynb](01%20Exact%20samplings.ipynb). Tools to encode discretized continuous functions into quantum states. Various 1D, 2D and 3D functions used throughout the work. Estimates of the entanglement in those encodings.\n",
    "- [02 MPS discussion.ipynb](02%20MPS%20discussion.ipynb). Algorithms for encoding continuous functions using MPS states. Analysis of this representation for squeezed Gaussian states in 2D and 3D.\n",
    "- [03 MPS Finite differences.ipynb](03%20MPS%20Finite%20differences.ipynb). As the name indicates, discussion of the encoding of differential operators and solution of Fokker-Planck equations using MPS.\n",
    "- [04 MPS Fourier methods.ipynb](04%20MPS%20Fourier%20methods.ipynb). Discussion of the implementation of Quantum Fourier Transforms using MPS, its use for interpolation and for solving PDEs.\n",
    "- [05 Plots.ipynb](05%20Plots.ipynb). Code to process the data computed by other notebooks into publication-ready plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repository is organized with the following structure\n",
    "- `./` The notebooks mentioned above and some helper scripts\n",
    "- `seeq` Automatically downloaded library\n",
    "- `data` Pickle files generated by the simulations of wavefunctions\n",
    "- `data-mps` Pickle files generated by the simulations of MPS states\n",
    "- `figures` Output directory for plots\n",
    "- `jobs` Jobs that we send to our cluster when simulations are particularly heavy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code download some required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making directories for mps\\version.py\n",
      "Writing file mps\\version.py\n",
      "Making directories for mps\\__init__.py\n",
      "Writing file mps\\__init__.py\n",
      "Making directories for mps\\tools.py\n",
      "Writing file mps\\tools.py\n",
      "Making directories for mps\\test\\tools.py\n",
      "Writing file mps\\test\\tools.py\n",
      "Making directories for mps\\test\\test_tools.py\n",
      "Writing file mps\\test\\test_tools.py\n",
      "Making directories for mps\\state.py\n",
      "Writing file mps\\state.py\n",
      "Making directories for mps\\test\\test_mps.py\n",
      "Writing file mps\\test\\test_mps.py\n",
      "Making directories for mps\\test\\test_sample_states.py\n",
      "Writing file mps\\test\\test_sample_states.py\n",
      "Making directories for mps\\test\\test_canonical.py\n",
      "Writing file mps\\test\\test_canonical.py\n",
      "Making directories for mps\\mpo.py\n",
      "Writing file mps\\mpo.py\n",
      "Making directories for mps\\expectation.py\n",
      "Writing file mps\\expectation.py\n",
      "Making directories for mps\\test\\test_expectation.py\n",
      "Writing file mps\\test\\test_expectation.py\n",
      "Making directories for mps\\truncate.py\n",
      "Writing file mps\\truncate.py\n",
      "Making directories for mps\\test\\test_truncate.py\n",
      "Writing file mps\\test\\test_truncate.py\n",
      "Making directories for mps\\hamiltonians.py\n",
      "Writing file mps\\hamiltonians.py\n",
      "Making directories for mps\\test\\test_hamiltonians.py\n",
      "Writing file mps\\test\\test_hamiltonians.py\n",
      "Making directories for mps\\test\\test_TEBD.py\n",
      "Writing file mps\\test\\test_TEBD.py\n",
      "Making directories for mps\\evolution.py\n",
      "Writing file mps\\evolution.py\n",
      "Making directories for mps\\qft.py\n",
      "Writing file mps\\qft.py\n",
      "Making directories for mps\\test\\test_qft.py\n",
      "Writing file mps\\test\\test_qft.py\n",
      "Making directories for mps\\register.py\n",
      "Writing file mps\\register.py\n",
      "Making directories for mps\\test\\test_register.py\n",
      "Writing file mps\\test\\test_register.py\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "if not os.path.exists('seemps'):\n",
    "    !git clone http://github.com/juanjosegarciaripoll/seemps\n",
    "!cd seemps && make.cmd all\n",
    "for d in ['data','mps-data','figures']:\n",
    "    if not os.path.exists(d):\n",
    "        os.mkdir(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a Windows script used for the following tasks\n",
    "1. `make all` extracts Python files from all notebooks. These files are used as tiny libraries in other parts, or they contain jobs that we submit to the cluster.\n",
    "2. `make clean` eliminates files that can be recreated easily. This include scripts, libraries and data files.\n",
    "3. `make cleanup` eliminates the output from all Jupyter notebooks. Only use it if you plan to run them all, which takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting make.cmd\n"
     ]
    }
   ],
   "source": [
    "%%writefile make.cmd\n",
    "@echo off\n",
    "if \"%1\" == \"all\" (\n",
    "    python -c \"import exportnb; import glob; exportnb.export_notebooks(glob.glob('*.ipynb'),verbose=True); quit()\"\n",
    ")\n",
    "if \"%1\" == \"clean\" (\n",
    "    rmdir /S /Q seeq\n",
    "    del /Q make.cmd core_mps.py core.py mpi*.py job*.py\n",
    ")\n",
    "if \"%1\" == \"cleanup\" (\n",
    "    for %%i in (*.ipynb); do jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace \"%%i\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making directories for mpijobs.py\n",
      "Writing file mpijobs.py\n",
      "Making directories for core.py\n",
      "Writing file core.py\n",
      "Making directories for core_mps.py\n",
      "Writing file core_mps.py\n",
      "Making directories for mpi_mps_entropies2d.py\n",
      "Writing file mpi_mps_entropies2d.py\n",
      "Making directories for job-2d-m5.py\n",
      "Writing file job-2d-m5.py\n",
      "Making directories for job-2d-m7.py\n",
      "Writing file job-2d-m7.py\n",
      "Making directories for job-2d-m9.py\n",
      "Writing file job-2d-m9.py\n",
      "Making directories for job-2d-m11.py\n",
      "Writing file job-2d-m11.py\n",
      "Making directories for job-2d-m14.py\n",
      "Writing file job-2d-m14.py\n",
      "Making directories for job-2d-m16.py\n",
      "Writing file job-2d-m16.py\n",
      "Making directories for job-2d-m18.py\n",
      "Writing file job-2d-m18.py\n",
      "Making directories for mpi_mps_entropies3d.py\n",
      "Writing file mpi_mps_entropies3d.py\n",
      "Making directories for job-3d-m5.py\n",
      "Writing file job-3d-m5.py\n",
      "Making directories for job-3d-m7.py\n",
      "Writing file job-3d-m7.py\n",
      "Making directories for job-3d-m9.py\n",
      "Writing file job-3d-m9.py\n",
      "Making directories for job-3d-m11.py\n",
      "Writing file job-3d-m11.py\n",
      "Making directories for job-3d-m12.py\n",
      "Writing file job-3d-m12.py\n",
      "Making directories for job-3d-m13.py\n",
      "Writing file job-3d-m13.py\n"
     ]
    }
   ],
   "source": [
    "!make.cmd all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to manage our MPI jobs. It is a stupid class that spreads a number of simulations over various processors that have been coordinated using `mpirun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: mpijobs.py\n",
    "from mpi4py import MPI\n",
    "import time\n",
    "import pickle\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "with io.StringIO() as buf, redirect_stdout(buf):\n",
    "    print('redirected')\n",
    "    output = buf.getvalue()\n",
    "\n",
    "class Manager(object):\n",
    "\n",
    "    def __init__(self, root=0, debug=False, root_computes=False):\n",
    "        self.comm = MPI.COMM_WORLD\n",
    "        self.inode = MPI.Get_processor_name()    # Node where this MPI process runs\n",
    "        self.size = self.comm.Get_size()   # Size of communicator\n",
    "        self.rank = self.comm.Get_rank()   # Ranks in communicator\n",
    "        self.root = root\n",
    "        self.isroot = self.rank == root\n",
    "        self.root_computes = root_computes\n",
    "        self.debug = debug\n",
    "        if debug:\n",
    "            name = 'master' if self.isroot else 'slave'\n",
    "            print(f'Running {self.inode} [{name}], rank={self.rank} out of {self.size}')\n",
    "\n",
    "    def partition(self, data):\n",
    "        if self.root_computes:\n",
    "            data = [data[i::self.size] for i in range(self.size)]\n",
    "        else:\n",
    "            data = [data[i::self.size-1] for i in range(self.size-1)]\n",
    "            data = data[0:self.root] + [[]] + data[self.root:]\n",
    "        return data\n",
    "\n",
    "    def run(self, job_description, file=None):\n",
    "        job_description = list(enumerate(job_description))\n",
    "        if self.isroot:\n",
    "            data = self.partition(job_description)\n",
    "        else:\n",
    "            data = None\n",
    "        data = self.comm.scatter(data, root=self.root)\n",
    "        if self.debug:\n",
    "            print(f'** Node {self.rank} received {len(data)} items {[order for order,_ in data]}')\n",
    "        data = [self.run_one(pair) for pair in data]\n",
    "        if self.debug:\n",
    "            print(f'** Node {self.rank} computed {len(data)} items')\n",
    "        data = self.comm.gather(data, root=self.root)\n",
    "        if self.isroot:\n",
    "            data = sorted(sum(data, []), key=lambda x: x[0])\n",
    "            if self.debug and self.isroot:\n",
    "                print(f'** Root {self.rank} gathered {len(data)} items (expected {len(job_description)})')\n",
    "            if file is not None:\n",
    "                try:\n",
    "                    clean = [value for order, time, value, text_output in data]\n",
    "                    with open(file, 'ab') as f:\n",
    "                        pickle.dump((clean, data), f)\n",
    "                    if self.debug:\n",
    "                        print(f'Master node {self.rank} saved data in {file}')\n",
    "                except:\n",
    "                    print(f'Unable to save data. Aborting')\n",
    "            if self.debug:\n",
    "                for order, time, _, text_output in data:\n",
    "                    print(f'-----\\nJob {order} output:')\n",
    "                    print(text_output)\n",
    "                    print(f'Ran in {time}s')\n",
    "\n",
    "    def run_one(self, pair):\n",
    "        order, job_item = pair\n",
    "        t = time.process_time()\n",
    "        output = ''\n",
    "        with io.StringIO() as buf:\n",
    "            with redirect_stdout(buf):\n",
    "                try:\n",
    "                    values = job_item[0](*job_item[1:])\n",
    "                except Exception as e:\n",
    "                    if debug:\n",
    "                        print(f'Exception raised: \"{e}\"')\n",
    "                    values = e\n",
    "            text_output = buf.getvalue()\n",
    "        return order, time.process_time() - t, values, text_output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
